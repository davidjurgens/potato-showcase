{
  "title": "InstructGPT Instruction Following",
  "description": "Evaluate how well AI responses follow user instructions. Compare outputs on helpfulness, truthfulness, and harmlessness for RLHF training.",
  "version": "1.0.0",
  "author": {
    "name": "Potato Team",
    "email": "jurgens@umich.edu",
    "github": "davidjurgens",
    "affiliation": "University of Michigan"
  },
  "createdAt": "2025-02-02",
  "updatedAt": "2025-02-02",
  "annotationTypes": ["pairwise", "likert", "radio"],
  "domain": ["Natural Language Processing", "AI Alignment"],
  "useCase": ["RLHF", "Instruction Following", "Preference Learning"],
  "complexity": "intermediate",
  "tags": ["preference", "instruction", "helpfulness", "rlhf", "alignment", "gpt"],
  "featured": false,
  "paperReference": "Ouyang et al., NeurIPS 2022",
  "paperUrl": "https://arxiv.org/abs/2203.02155",
  "datasetUrl": "https://github.com/openai/following-instructions-human-feedback",
  "citation": "@article{ouyang2022training,\n  title={Training language models to follow instructions with human feedback},\n  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},\n  journal={Advances in Neural Information Processing Systems},\n  volume={35},\n  pages={27730--27744},\n  year={2022}\n}"
}
