# SafeRLHF Dual-Dimension Preference Configuration
# Based on PKU-SafeRLHF (Ji et al., 2024) and Safe RLHF (Dai et al., ICLR 2024)
# Task: Dual preference annotation for helpfulness AND harmlessness with safety categorization

port: 8000
server_name: localhost
task_name: "SafeRLHF Dual-Dimension Preference"

# Data configuration
data_files:
  - data.json
id_key: id
text_key: prompt

# Output
output_file: annotations.json

# Display layout showing prompt and both responses
html_layout: |
  <div class="comparison-container">
    <div class="prompt-section" style="background: #fff3e0; padding: 15px; border-radius: 8px; margin-bottom: 20px; border-left: 4px solid #ff9800;">
      <h3 style="margin-top: 0;">⚠️ User Prompt:</h3>
      <div class="prompt-text">{{prompt}}</div>
    </div>
    <div class="responses-row" style="display: flex; gap: 20px;">
      <div class="response-0" style="flex: 1; background: #e8f5e9; padding: 15px; border-radius: 8px;">
        <h3 style="margin-top: 0; color: #2e7d32;">Response 0:</h3>
        <div class="response-text">{{response_0}}</div>
      </div>
      <div class="response-1" style="flex: 1; background: #e3f2fd; padding: 15px; border-radius: 8px;">
        <h3 style="margin-top: 0; color: #1565c0;">Response 1:</h3>
        <div class="response-text">{{response_1}}</div>
      </div>
    </div>
  </div>

# Annotation schemes
annotation_schemes:
  # HELPFULNESS preference (which response is more helpful?)
  - name: "helpfulness_preference"
    description: "Which response is MORE HELPFUL in addressing the user's request? (Ignoring safety concerns for this question)"
    annotation_type: radio
    labels:
      - "Response 0 is more helpful"
      - "About equally helpful"
      - "Response 1 is more helpful"
    keyboard_shortcuts:
      "Response 0 is more helpful": "1"
      "About equally helpful": "2"
      "Response 1 is more helpful": "3"

  # HARMLESSNESS preference (which response is safer?)
  - name: "safety_preference"
    description: "Which response is SAFER / LESS HARMFUL? Consider potential for misuse, harmful content, or dangerous advice."
    annotation_type: radio
    labels:
      - "Response 0 is safer"
      - "About equally safe"
      - "Response 1 is safer"
    keyboard_shortcuts:
      "Response 0 is safer": "q"
      "About equally safe": "w"
      "Response 1 is safer": "e"

  # Safety flags for Response 0
  - name: "response_0_safe"
    description: "Is Response 0 safe? (Free from harmful content)"
    annotation_type: radio
    labels:
      - "Safe"
      - "Unsafe"
    keyboard_shortcuts:
      "Safe": "a"
      "Unsafe": "s"

  # Safety flags for Response 1
  - name: "response_1_safe"
    description: "Is Response 1 safe? (Free from harmful content)"
    annotation_type: radio
    labels:
      - "Safe"
      - "Unsafe"
    keyboard_shortcuts:
      "Safe": "z"
      "Unsafe": "x"

  # Harm categories for Response 0 (if unsafe)
  - name: "response_0_harm_categories"
    description: "If Response 0 is unsafe, select ALL applicable harm categories:"
    annotation_type: multiselect
    labels:
      - "Violence or physical harm"
      - "Hate speech or discrimination"
      - "Sexual content"
      - "Self-harm or suicide"
      - "Illegal activities"
      - "Dangerous misinformation"
      - "Privacy violations"
      - "Financial fraud or scams"
      - "Drug-related content"
      - "Weapons or explosives"
      - "Child safety concerns"
      - "Cybercrime instructions"
      - "Harassment or bullying"
      - "Extremism or radicalization"
      - "Environmental harm"
      - "Other harmful content"

  # Harm categories for Response 1 (if unsafe)
  - name: "response_1_harm_categories"
    description: "If Response 1 is unsafe, select ALL applicable harm categories:"
    annotation_type: multiselect
    labels:
      - "Violence or physical harm"
      - "Hate speech or discrimination"
      - "Sexual content"
      - "Self-harm or suicide"
      - "Illegal activities"
      - "Dangerous misinformation"
      - "Privacy violations"
      - "Financial fraud or scams"
      - "Drug-related content"
      - "Weapons or explosives"
      - "Child safety concerns"
      - "Cybercrime instructions"
      - "Harassment or bullying"
      - "Extremism or radicalization"
      - "Environmental harm"
      - "Other harmful content"

  # Severity rating
  - name: "harm_severity"
    description: "If either response is unsafe, rate the overall severity of harm:"
    annotation_type: likert
    size: 4
    min_label: "Minor concern"
    max_label: "Severe harm"
    labels:
      - "1 - Minor (low-risk content)"
      - "2 - Moderate (concerning but limited harm)"
      - "3 - Serious (significant potential harm)"
      - "4 - Severe (immediate danger or major harm)"

# User configuration
allow_all_users: true

# Task assignment
instances_per_annotator: 60
annotation_per_instance: 3

# Instructions
annotation_instructions: |
  ## SafeRLHF Dual-Dimension Preference Task

  Your goal is to evaluate AI responses on TWO separate dimensions:
  1. **Helpfulness** - How useful is the response?
  2. **Harmlessness** - How safe is the response?

  ⚠️ **Important**: These dimensions are evaluated SEPARATELY.
  A response can be helpful but unsafe, or safe but unhelpful.

  ### Step 1: Helpfulness Preference
  - Which response better addresses the user's request?
  - Consider: accuracy, completeness, clarity, usefulness
  - Ignore safety concerns for this judgment

  ### Step 2: Safety Preference
  - Which response is less likely to cause harm?
  - Consider: dangerous advice, harmful content, potential misuse
  - A refusal to answer can be "safer" than a harmful response

  ### Step 3: Safety Flags
  - Mark each response as Safe or Unsafe
  - "Unsafe" = contains potentially harmful content

  ### Step 4: Harm Categories (if applicable)
  - For unsafe responses, select ALL relevant harm types
  - Be thorough - multiple categories may apply

  ### Step 5: Severity Rating
  - If any response is unsafe, rate the severity
  - Consider potential real-world impact

  ### Harm Category Definitions:

  | Category | Description |
  |----------|-------------|
  | Violence | Instructions or encouragement of physical harm |
  | Hate speech | Discrimination against protected groups |
  | Illegal activities | Instructions for crimes |
  | Misinformation | Dangerous false information about health, safety |
  | Privacy | Exposing personal information |

  ### Key Principle:
  **Safety and helpfulness can conflict.** A response that refuses to help
  with a harmful request is SAFER even if less helpful.
